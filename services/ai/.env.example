# Example environment configuration file
# Copy this to .env and update with your actual values

# Server configuration
HOST=0.0.0.0
PORT=1
LOG_LEVEL=INFO
DEBUG=false
ENVIRONMENT=development

# API configuration
API_TITLE=AgentChat API
API_DESCRIPTION=API for interacting with the AI agent
API_VERSION=0.2.0

# Frontend URL for CORS
# For local development: http://localhost:3000
# For production with ngrok: https://your-ngrok-url.ngrok-free.app,https://yourproductiondomain.com
FRONTEND_URL=http://localhost:3000

# Google Cloud Platform configuration (required)
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_LOCATION=your-gcp-location
GCS_BUCKET_NAME=agentchat-data
SERVICE_ACCOUNT_EMAIL=your-service-account
SIGNED_URL_LIFETIME=2

# Model configuration
GEMINI_MODEL=gemini-2.5-flash
GEMINI_MODEL_PRO=gemini-2.5-pro

# Model provider: 'gemini' or 'ollama'
MODEL_PROVIDER=gemini

# Ollama configuration (used when MODEL_PROVIDER=ollama)
# Make sure Ollama is running locally with: ollama serve
OLLAMA_API_BASE=http://localhost:11434
OLLAMA_MODEL=mistral-small3.1
OLLAMA_MODEL_PRO=llama3.2

# Authentication
AUTH_SECRET=your-super-secret-key

# Development settings
RESTART_SCRIPT_PATH=./scripts/restart-server.sh

# --- Legacy configuration (for reference) ---
# The following variables may still be used by other parts of the system

# Model access: Google AI Studio API or Vertex AI API(through GCP project)
GOOGLE_GENAI_USE_VERTEXAI=FALSE

# For Google AI Studio API (GOOGLE_GENAI_USE_VERTEXAI=FALSE)
GOOGLE_API_KEY=${GEMINI_API_KEY}
